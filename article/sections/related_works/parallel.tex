\subsection{Parallel Software}

In \cite{abdelhafez2019}, the energy consumption for sequential and parallel genetic algorithms is explored, where one research question aims to explore the impact on energy consumption when using different numbers of cores. They find that a larger number of cores in the execution pool results in a lower running time and energy consumption, and conclude that parallelism can help reduce energy consumption. Parallelism's ability to reduce energy consumption is argued to be due to the large number of cores working to solve the problem simultaneously, where the combination of more cores, and more parallel operations per time unit will require less energy.

When considering parallel software, \cite{abdelhafez2019} also find asynchronous implementations to use less energy, because there are no idle cores waiting for data in asynchronous implementations, while in synchronous implementations cores can be blocked during runtime, while waiting for responses from other cores. 

% RAPL, C++, 30 runs

In \cite{Pinto2014}, three thread management constructs, explicit thread creation, fixed-size thread pooling and work stealing, from Java are analyzed in regards to energy consumption. Furthermore the three aspects, number of threads, task division strategy and characteristics of the data. 

They found that in 8 our of 9 benchmarks there was an decrease in execution time going from sequential execution on one thread to using multiple threads. However it should be noted that four of their benchmarks are embarrassingly Parallel where as only one is embarrassingly serial. However decreased execution time does not necessarily mean decreased energy consumption.



In \cite{Lindholt}, the behavior of parallel applications and the relationship between execution time and energy consumption are explored. They test four different language constructs which can be used to implement parallelism in C\#. Furthermore, they use varying amounts of threads and a sample of micro- and macro-benchmarks. \cite{Lindholt}

They found that workload size has a large influence on running time and energy efficiency and that a threshold limit for the workload must be reached for there to be improvements when changing a sequential program into a parallel one.  Additionally, it was found that execution time and energy consumption of parallel benchmarks do not always correlate.  Comparing micro- and macro-benchmarks the findings remain consistent, although the impact becomes low for the macrobenchmarks due to their being an overall larger energy consumption. Furthermore, they have included some recommendations, which should be considered:\cite{Lindholt}

\begin{itemize}
    \item Shield cores: Avoid unintended threads running on the cores used in the benchmarking
    \item PowerUp: Can be used to ensure that benchmark is not optimized away during compilation
    \item Static clock: Make the clock rate  of the CPU as static as possible
    \item Interrupt request: Avoid interrupt requests being sent to cores used in the benchmarking 
    \item Turn off CPU turbo boost
    \item Turn off hyperthreading
\end{itemize}