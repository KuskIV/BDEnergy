\subsection{Parallel Software}

Amdahl's law describes the potential speedup achieved by running an algorithm in parallel based on the proportion of the algorithm that can be parallelized and the number of cores used.\cite{amdahl1967validity} In \cite{woo2008extending}, Amdahl's law was extended to estimate energy consumption with different amounts of cores. It was argued that a CPU could lose its energy efficiency as the number of cores increased and that knowing how parallelizable a program is before execution allowed for calculating the optimal number of active cores for maximizing performance and energy consumption.\cite{woo2008extending}

\cite{prinslow2011overview} compared the observed speedup of computing Laplace equations with one, two, and four cores, with estimates given by Amdahl's law and Gustafson's law. Gustafson's law evaluates a parallel program's speedup based on the problem's size and the number of cores. Unlike Amdahl's law which assumed a fixed problem size and a fixed proportion of the program that could be parallelized, Gustafson's law takes into account that larger problems could be solved when more cores are available and that the parallelization of a program could scale with the problem size. Comparing the observed and estimated speedup, it was clear that Gustafson's law was more optimistic than Amdahl's law, where both underestimated the speedup on two and four cores.\cite{prinslow2011overview}

% Amdahl's law describes the potential speedup achieved by running an algorithm in parallel based on the proportion of the algorithm that can be parallelized and the number of cores used.\cite{amdahl1967validity} In \cite{woo2008extending} Amdahl's law, was extended to also estimate the energy consumption and tested on three different many-core designs with different number of cores. The comparison showed that a CPU could lose its energy efficiency as the number of cores increased and it was argued that knowing how parallelizable a program is before execution allowed for calculating the optimal number of active cores for maximizing performance and energy consumption.\cite{woo2008extending}

% \cite{prinslow2011overview} compared the observed speedup of computing Laplace equations with one, two, and four cores, with estimates given by Amdahl's law and Gustafson's law. Gustafson's law evaluates the speedup of a parallel program based on the size of the problem and the number of cores. Unlike Amdahl's law which assumed a fixed problem size and a fixed proportion of the program that could be parallelized, Gustafson's law takes into account that larger problems could be solved when more cores are available and that the parallelization of a program could scale with the problem size. Comparing the observed and estimated speedup it was clear that Gustafson's law was more optimistic than Amdahl's law, where both underestimated the speedup on two and four cores.\cite{prinslow2011overview}

% RAPL, C++, 30 runs
%, explicit thread creation, fixed-size thread pooling, and work stealing, from Java, are analyzed regarding energy consumption. Furthermore, the three aspects: the number of threads, task division strategy, and characteristics of the data.\cite{Pinto2014}

In \cite{Pinto2014}, three different thread management constructs from Java were explored and analyzed. When allocating additional threads, the energy consumption was found to increase until a certain point where the energy consumption would start to decrease. The exact peak point was, however, found to be application-dependent. The study also found that in eight out of nine benchmarks, there was a decrease in execution time when transitioning from sequential execution on one thread to using multiple threads. It should be noted that four of their benchmarks were embarrassingly parallel, while only one was embarrassingly serial. The results also showed how a lower execution time does not imply a lower energy consumption, which was the case in six out of nine benchmarks.\cite{Pinto2014}

% Moreover, decreased execution time does not necessarily imply decreased energy consumption, because in six out of nine benchmarks, the lowest energy consumption was found in the sequential version.\cite{Pinto2014}% Furthermore, the study used Energy-Delay-Product, the product of energy consumption and execution time and found that in general parallel execution was favorable. However, increasing the number of threads was not an improvement for all of the benchmarks.\cite{Pinto2014}
% This website says that EDP is not adequate https://greensoftware.foundation/articles/gps-up-a-better-metric-for-comparing-software-energy-efficiency

\cite{abdelhafez2019} found that a larger amount of cores in the execution pool resulted in a lower running time and energy consumption and concluded that parallelism could help reduce energy consumption for genetic algorithms. %Parallelism's ability to reduce energy consumption was argued to be due to the large number of cores working to solve the problem simultaneously, where the combination of more cores, and more parallel operations per time unit will require less energy. 
% When considering parallel software, it also found asynchronous implementations to use less energy, because there are no idle cores waiting for data in asynchronous implementations, while in synchronous implementations cores can be blocked during runtime, while waiting for responses from other cores.

In \cite{Lindholt2022}, %the behavior of parallel applications and the relationship between execution time and energy consumption are explored. It tests 
In \cite{Lindholt2022}, four different language constructs used to implement parallelism in C\# were tested by executing a set of micro- and macrobenchmarks on a different number of treads. It was found that workload size greatly influenced run time and energy efficiency. A limit was found for a sequential program, after which changing it to executing in parallel would be beneficial. The findings remained consistent between micro- and macrobenchmarks. However, the impact was less significant for the macrobenchmarks due to a higher total energy consumption.% In \cite{Lindholt2022} a set of recommendations are introduced when performing energy measurements, which include that the clock rate of the CPU as static as possible, Turbo Boost should be disabled, and hyperthreading should be turned off.


% Furthermore, it has included some recommendations, which are considered in our setup\cite{Lindholt2022}

% \begin{itemize}
%     %\item Shield cores: Avoid unintended threads running on the cores used in the benchmarking
%     %\item PowerUp: Can be used to ensure that benchmark is not optimized away during compilation
%     \item Static clock: Make the clock rate of the CPU as static as possible
%     %\item Interrupt request: Avoid interrupt requests being sent to cores used in the benchmarking 
%     \item Turn off CPU turbo boost
%     \item Turn off hyperthreading
% \end{itemize}